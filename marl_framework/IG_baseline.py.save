import copy
from typing import Dict

import numpy as np
from matplotlib import pyplot as plt
from torch.utils.tensorboard import SummaryWriter

from logger import setup_logger
import constants
from agent.action_space import AgentActionSpace
from agent.agent import Agent
from agent.state_space import AgentStateSpace
from batch_memory import BatchMemory
from coma_wrapper import COMAWrapper
from mapping.grid_maps import GridMap
from mapping.mappings import Mapping
from mapping.simulations import Simulation
from params import load_params
from sensors import Sensor
from sensors.cameras import Camera
from sensors.models import SensorModel
from sensors.models.sensor_models import AltitudeSensorModel
from utils.plotting import plot_trajectories
from utils.reward import get_global_reward
from utils.state import get_shannon_entropy


class IG_baseline:
    def __init__(self, params: Dict, writer: SummaryWriter, num_episode):
        self.params = params
        self.num_episode = num_episode
        self.budget = params["experiment"]["constraints"]["budget"]
        self.n_agents = params["experiment"]["missions"]["n_agents"]
        self.class_weighting = params["experiment"]["missions"]["class_weighting"]
        self.coma_wrapper = COMAWrapper(params, writer)
        self.grid_map = GridMap(params)
        self.sensor_model = AltitudeSensorModel(params)
        self.sensor = Sensor(self.sensor_model, self.grid_map)
        self.mapping = Mapping(self.grid_map, self.sensor, self.params, num_episode)
        self.agent_state_space = AgentStateSpace(params)
        self.action_space = AgentActionSpace(params)
        self.batch_memory = BatchMemory(params, self.coma_wrapper)
        self.camera = Camera(params, self.sensor_model, self.grid_map)
        self.simulation = Simulation(params, self.sensor, num_episode, self.sensor_model)
        self.writer = writer

    def execute(self):
        agents = []
        for agent_id in range(self.n_agents):
            agents.append(
                Agent(
                    self.coma_wrapper.actor_network,
                    self.params,
                    self.mapping,
                    agent_id,
                    self.agent_state_space,
                )
            )
        next_maps = []
        # next_positions = []
        agent_positions = []
        agent_altitudes = []
        relative_rewards = []
        absolute_rewards = []

        for t in range(self.budget + 1):
            action_position_list = []
            information_gain_list = []
            next_positions = []
            altitudes = []

            _, fused_map, positions, observations = self.coma_wrapper.build_observations(
                self.mapping, agents, self.num_episode, t, self.params, self.batch_memory, None
            )

            print(f"t: {t}")
            print(f"fused_map: {dict((i, fused_map.count(i)) for i in fused_map)}")

            if t == 0:
                agent_positions.append(positions)

            for agent_id in range(self.n_agents):
                action_mask = self.action_space.get_action_mask(agents[agent_id].position)[0]
                action_positions, information_gains = self.get_individual_ig(agents[agent_id].position, action_mask,
                                                                             agents[agent_id].local_map)
                action_position_list.append(action_positions)
                information_gain_list.append(information_gains)

                # print(f"action_positions: {action_positions}")
                # print(f"information_gains: {information_gains}")

            relative_information_gains = self.get_relative_ig(information_gain_list)
            cell_utilities = self.get_cell_utilities(action_position_list, relative_information_gains)

            for agent_id in range(self.n_agents):
                action = self.select_action(cell_utilities[agent_id])
                agents[agent_id].position = self.action_space.action_to_position(agents[agent_id].position, action)

                # print(f"action: {action}")
                # print(f"new position: {agents[agent_id].position}")

                agents[agent_id].local_map, _, _, _ = self.mapping.update_grid_map(agents[agent_id].position,
                                                                                agents[agent_id].local_map, t, None)
                next_maps.append(agents[agent_id].local_map)
                next_positions.append(agents[agent_id].position)
                altitudes.append(agents[agent_id].position[2])

            agent_positions.append(next_positions)
            agent_altitudes.append(altitudes)
            next_fused_map = self.mapping.fuse_map(next_maps[0], next_maps[1:], None)

            # print(f"next_fused_map: {next_fused_map}")

            done, relative_reward, absolute_reward = get_global_reward(fused_map, next_fused_map, None, None, self.mapping.simulated_map,
                                                self.agent_state_space, None, None)

            print(f"absolute_reward: {absolute_reward}")

            relative_rewards.append(relative_reward)
            absolute_rewards.append(absolute_reward)
        plot_trajectories(agent_positions, self.n_agents, self.writer, self.num_episode, None, None, self.mapping.simulated_map)

        return sum(relative_rewards), sum(absolute_rewards), agent_altitudes

    def get_individual_ig(self, position, action_mask, map_state):
        action_positions = []
        information_gains = []

        # print(f"position: {position}")

        for action in range(len(action_mask))
            if action_mask[action] == 0:
                action_positions.append(0)
                information_gains.append(0)
            else:
                new_position = self.action_space.action_to_position(position, action)
                footprint = self.camera.project_field_of_view(new_position, self.grid_map.resolution_x,
                                                              self.grid_map.resolution_y)

                # print(f"new_position: {new_position}")

                map_section = map_state[footprint[2]: footprint[3], footprint[0]: footprint[1]]
                noise_level = self.sensor_model.get_noise_variance(new_position[2])
                class_weightings_1 = self.mapping.update_cells(map_section.copy(), 1 - noise_level, None)
                class_weightings_2 = self.mapping.update_cells(map_section.copy(), noise_level, None)
                class_weightings_1[class_weightings_1 > 0.501] = 1
                class_weightings_1[class_weightings_1 < 0.501] = 0.5
                class_weightings_1[class_weightings_1 < 0.409] = 0
                class_weightings_2[class_weightings_2 > 0.501] = 1
                class_weightings_2[class_weightings_2 < 0.501] = 0.5
                class_weightings_2[class_weightings_2 < 0.409] = 0

                ig = map_section * (get_shannon_entropy(map_section) - get_shannon_entropy(self.mapping.update_cells(map_section, 1 - noise_level, None))) * class_weightings_1 + (
                        1 - map_section) * (get_shannon_entropy(map_section) - get_shannon_entropy(self.mapping.update_cells(map_section, noise_level, None))) * class_weightings_2
                total_ig = np.sum(ig) / 1000

                action_positions.append(new_position)
                information_gains.append(total_ig)
        return action_positions, information_gains

    def get_relative_ig(self, information_gain_list):
        for agent_id in range(len(information_gain_list)):
            total_ig = sum(information_gain_list[agent_id])
            for pos in range(len(information_gain_list[agent_id])):
                information_gain_list[agent_id][pos] = information_gain_list[agent_id][pos] / total_ig
        return information_gain_list

    def get_cell_utilities(self, action_position_list, relative_information_gains):
        for agent_id in range(len(action_position_list)):
            for pos1 in range(len(action_position_list[agent_id])):
                position1 = action_position_list[agent_id][pos1]
                relative_information_gain1 = relative_information_gains[agent_id][pos1]
                for id2 in range(len(action_position_list)):
                    if id2 == agent_id:
                        pass
                    else:
                        for pos2 in range(len(action_position_list[id2])):
                            position2 = action_position_list[id2][pos2]
                            relative_information_gain2 = relative_information_gains[id2][pos2]
                            if np.array_equal(position1, position2):
                                if not type(position1) is np.ndarray:
                                    pass
                                else:
                                    relative_information_gains[agent_id][pos1] = relative_information_gain1 * (1 - relative_information_gain2)
        return relative_information_gains

    def select_action(self, cell_utilities):
        return np.argmax(cell_utilities)


def main():
    constants.log_env_variables()
    params = load_params(constants.CONFIG_FILE_PATH)
    writer = SummaryWriter(constants.LOG_DIR)
    trials = params["experiment"]["baselines"]["information_gain"]["trials"]
    returns = []
    altitude_list = []
    for trial in range(1, trials + 1):
        ig_baseline = IG_baseline(params, writer, trial)
        _, global_return, agent_altitudes = ig_baseline.execute()
        altitude_list.append([item for sublist in agent_altitudes for item in sublist])
        print(f"Trial {trial}: {global_return}")

        returns.append(global_return)
    mean_return = sum(returns) / len(returns)
    max_return = max(returns)
    min_return = min(returns)
    std_return = np.std(np.array(returns))
    print(f"Mean return of {trials} trials: {mean_return}")
    print(f"Max return of {trials} trials: {max_return}")
    print(f"Min return of {trials} trials: {min_return}")
    print(f"Std dev return of {trials} trials: {std_return}")

    altitude_list = [item for sublist in altitude_list for item in sublist]
    altitude_counts = [altitude_list.count(i) for i in [5, 10, 15]]
    altitude_ratio = [(i / sum(altitude_counts)) for i in altitude_counts]
    print(f"Altitude_counts: {altitude_counts}")
    print(f"Altitude_ratio: {altitude_ratio}")


if __name__ == "__main__":
    main()
