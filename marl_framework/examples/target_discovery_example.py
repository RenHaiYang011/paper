"""
ç›®æ ‡å‘ç°å¥–åŠ±æœºåˆ¶ä½¿ç”¨ç¤ºä¾‹

æœ¬ç¤ºä¾‹å±•ç¤ºå¦‚ä½•ä½¿ç”¨æ–°çš„ç›®æ ‡å‘ç°å¥–åŠ±æœºåˆ¶ï¼š
1. ç›®æ ‡å‘ç°å¥–åŠ±
2. ä»»åŠ¡å®Œæˆ/å¤±è´¥å¥–åŠ±  
3. ååŒå‘ç°å¥–åŠ±
4. æ”¹è¿›çš„çŠ¶æ€è¡¨ç¤º
"""

import numpy as np
import yaml
from marl_framework.utils.reward import (
    get_global_reward,
    detect_new_target_discoveries,
    calculate_collaborative_discovery_reward
)
from marl_framework.agent.state_space import AgentStateSpace
from marl_framework.actor.transformations import (
    get_network_input,
    get_discovery_history_map,
    get_exploration_intensity_map
)


def load_enhanced_config():
    """åŠ è½½å¢å¼ºçš„é…ç½®æ–‡ä»¶"""
    config = {
        "environment": {
            "seed": 42,
            "x_dim": 100,
            "y_dim": 100
        },
        "experiment": {
            "constraints": {
                "spacing": 5,
                "min_altitude": 5,
                "max_altitude": 25,
                "budget": 50,
                "num_actions": 6
            },
            "missions": {
                "n_agents": 4,
                "class_weighting": [0, 1],
                "planning_uncertainty": "SE"
            },
            # æ–°å¢çš„ç›®æ ‡å‘ç°å¥–åŠ±é…ç½®
            "target_discovery_reward": 50.0,
            "mission_success_reward": 100.0,
            "mission_failure_penalty": -50.0,
            "collaborative_discovery_weight": 25.0,
            "discovery_threshold": 0.8
        },
        # æ–°å¢çš„çŠ¶æ€è¡¨ç¤ºé…ç½®
        "state_representation": {
            "use_discovery_history": True,
            "use_exploration_intensity": True,
            "use_target_probability": True,
            "exploration_decay_factor": 0.9,
            "discovery_influence_radius": 2
        }
    }
    return config


def simulate_target_discovery_scenario():
    """æ¨¡æ‹Ÿç›®æ ‡å‘ç°åœºæ™¯"""
    print("=== ç›®æ ‡å‘ç°å¥–åŠ±æœºåˆ¶æ¼”ç¤º ===\n")
    
    # 1. åˆå§‹åŒ–é…ç½®
    config = load_enhanced_config()
    agent_state_space = AgentStateSpace(config)
    
    # 2. åˆ›å»ºæ¨¡æ‹Ÿåœ°å›¾å’Œæ™ºèƒ½ä½“çŠ¶æ€
    map_size = (20, 20)  # ç®€åŒ–çš„åœ°å›¾å°ºå¯¸
    
    # åˆ›å»ºåŒ…å«3ä¸ªç›®æ ‡çš„çœŸå®åœ°å›¾
    simulated_map = np.zeros(map_size)
    target_positions = [(5, 5), (10, 15), (15, 8)]
    for pos in target_positions:
        simulated_map[pos] = 1.0
    
    print(f"åˆ›å»ºäº†åŒ…å« {len(target_positions)} ä¸ªç›®æ ‡çš„åœ°å›¾")
    print(f"ç›®æ ‡ä½ç½®: {target_positions}\n")
    
    # 3. æ¨¡æ‹Ÿå‘ç°è¿‡ç¨‹
    discovered_targets = set()
    total_targets = len(target_positions)
    
    # æ™ºèƒ½ä½“ä½ç½®
    agent_positions = [
        np.array([25, 25, 10]),  # Agent 0
        np.array([50, 25, 15]),  # Agent 1  
        np.array([25, 75, 20]),  # Agent 2
        np.array([75, 75, 25])   # Agent 3
    ]
    
    # 4. æ¨¡æ‹Ÿå¤šä¸ªæ—¶é—´æ­¥çš„å‘ç°è¿‡ç¨‹
    for t in range(10):
        print(f"\n--- æ—¶é—´æ­¥ {t} ---")
        
        # æ¨¡æ‹Ÿåœ°å›¾æ›´æ–°ï¼ˆæ™ºèƒ½ä½“é€æ¸å‘ç°ç›®æ ‡ï¼‰
        last_map = np.random.rand(*map_size) * 0.3  # ä¹‹å‰çš„è§‚æµ‹
        next_map = np.random.rand(*map_size) * 0.3  # å½“å‰è§‚æµ‹
        
        # æ¨¡æ‹Ÿåœ¨æŸäº›æ—¶é—´æ­¥å‘ç°ç›®æ ‡
        if t == 3:
            # å‘ç°ç¬¬ä¸€ä¸ªç›®æ ‡
            next_map[5, 5] = 0.9  # é«˜ç½®ä¿¡åº¦
            print("ğŸ¯ Agent 0 å‘ç°äº†ç¬¬ä¸€ä¸ªç›®æ ‡!")
            
        elif t == 7:
            # å‘ç°ç¬¬äºŒä¸ªç›®æ ‡
            next_map[10, 15] = 0.85
            print("ğŸ¯ Agent 1 å‘ç°äº†ç¬¬äºŒä¸ªç›®æ ‡!")
            
        # æ£€æµ‹æ–°å‘ç°
        new_discoveries = detect_new_target_discoveries(
            last_map, next_map, simulated_map, agent_state_space, 
            discovered_targets, discovery_threshold=0.8
        )
        
        if new_discoveries > 0:
            print(f"âœ¨ æ–°å‘ç° {new_discoveries} ä¸ªç›®æ ‡!")
            
            # è®¡ç®—ååŒå‘ç°å¥–åŠ±
            discovering_agent = 0 if t == 3 else 1
            collab_rewards = calculate_collaborative_discovery_reward(
                discovering_agent, agent_positions, agent_positions[discovering_agent]
            )
            print(f"ğŸ¤ ååŒå‘ç°å¥–åŠ±: {collab_rewards}")
        
        # 5. è®¡ç®—ç»¼åˆå¥–åŠ±
        for agent_id in range(len(agent_positions)):
            reward_result = get_global_reward(
                last_map=last_map,
                next_map=next_map,
                mission_type="COMA",
                footprints=None,
                simulated_map=simulated_map,
                agent_state_space=agent_state_space,
                actions=None,
                agent_id=agent_id,
                t=t,
                budget=config["experiment"]["constraints"]["budget"],
                next_positions=agent_positions,
                # æ–°çš„ç›®æ ‡å‘ç°å‚æ•°
                target_discovery_reward=config["experiment"]["target_discovery_reward"],
                mission_success_reward=config["experiment"]["mission_success_reward"],
                mission_failure_penalty=config["experiment"]["mission_failure_penalty"],
                discovered_targets=discovered_targets,
                total_targets=total_targets
            )
            
            done, relative_reward, absolute_reward = reward_result
            
            if new_discoveries > 0 and agent_id == (0 if t == 3 else 1):
                print(f"ğŸ’° Agent {agent_id} å¥–åŠ±: {absolute_reward:.2f} (åŒ…å«å‘ç°å¥–åŠ±)")
            
            if done:
                if len(discovered_targets) >= total_targets:
                    print(f"ğŸ† ä»»åŠ¡æˆåŠŸå®Œæˆ! Agent {agent_id} è·å¾—æˆåŠŸå¥–åŠ±")
                else:
                    print(f"ğŸ’” ä»»åŠ¡å¤±è´¥! Agent {agent_id} å—åˆ°å¤±è´¥æƒ©ç½š")
                break
    
    print(f"\n=== æœ€ç»ˆç»“æœ ===")
    print(f"å‘ç°ç›®æ ‡æ•°é‡: {len(discovered_targets)} / {total_targets}")
    print(f"å‘ç°ç‡: {len(discovered_targets)/total_targets*100:.1f}%")


def demonstrate_enhanced_state_representation():
    """æ¼”ç¤ºå¢å¼ºçš„çŠ¶æ€è¡¨ç¤ºåŠŸèƒ½"""
    print("\n=== å¢å¼ºçŠ¶æ€è¡¨ç¤ºæ¼”ç¤º ===\n")
    
    # æ¨¡æ‹Ÿå·²å‘ç°çš„ç›®æ ‡
    discovered_targets = {(5, 5), (10, 15)}
    map_size = (20, 20)
    
    # åˆ›å»ºAgentStateSpace (ç®€åŒ–)
    config = load_enhanced_config()
    agent_state_space = AgentStateSpace(config)
    position_map = np.ones(map_size)
    
    # 1. ç›®æ ‡å‘ç°å†å²å›¾
    discovery_map = get_discovery_history_map(
        discovered_targets, agent_state_space, position_map
    )
    print("ğŸ“ ç”Ÿæˆç›®æ ‡å‘ç°å†å²å›¾")
    print(f"   - å·²å‘ç°ç›®æ ‡æ•°é‡: {len(discovered_targets)}")
    print(f"   - å†å²å›¾ä¸­éé›¶ä½ç½®: {np.sum(discovery_map > 0)}")
    
    # 2. æ¢ç´¢å¼ºåº¦å›¾
    # æ¨¡æ‹Ÿæœ¬åœ°ä¿¡æ¯
    local_information = {
        0: {"map2communicate": np.random.rand(*map_size)},
        1: {"map2communicate": np.random.rand(*map_size)},
        2: {"map2communicate": np.random.rand(*map_size)},
        3: {"map2communicate": np.random.rand(*map_size)}
    }
    
    intensity_map = get_exploration_intensity_map(
        local_information, 0, agent_state_space, position_map, 5
    )
    print("ğŸ” ç”Ÿæˆæ¢ç´¢å¼ºåº¦å›¾")
    print(f"   - å¹³å‡æ¢ç´¢å¼ºåº¦: {np.mean(intensity_map):.3f}")
    print(f"   - æœ€é«˜æ¢ç´¢å¼ºåº¦: {np.max(intensity_map):.3f}")
    
    print("\nâœ… çŠ¶æ€è¡¨ç¤ºå¢å¼ºå®Œæˆ!")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš å¤šæ™ºèƒ½ä½“ç›®æ ‡å‘ç°ç³»ç»Ÿæ¼”ç¤º\n")
    
    # 1. æ¼”ç¤ºç›®æ ‡å‘ç°å¥–åŠ±æœºåˆ¶
    simulate_target_discovery_scenario()
    
    # 2. æ¼”ç¤ºå¢å¼ºçš„çŠ¶æ€è¡¨ç¤º
    demonstrate_enhanced_state_representation()
    
    print("\nğŸ‰ æ¼”ç¤ºå®Œæˆ!")
    print("\nä¸»è¦æ”¹è¿›ç‚¹:")
    print("1. âœ… ç›®æ ‡å‘ç°å¥–åŠ± - è§£å†³ç¨€ç–å¥–åŠ±é—®é¢˜")
    print("2. âœ… ä»»åŠ¡å®Œæˆ/å¤±è´¥å¥–åŠ± - æ˜ç¡®çš„æˆåŠŸ/å¤±è´¥ä¿¡å·") 
    print("3. âœ… ååŒå‘ç°å¥–åŠ± - COMAä¿¡ç”¨åˆ†é…æœºåˆ¶")
    print("4. âœ… å¢å¼ºçŠ¶æ€è¡¨ç¤º - å‘ç°å†å²å’Œæ¢ç´¢å¼ºåº¦")
    print("5. âœ… é…ç½®åŒ–å‚æ•° - æ˜“äºè°ƒèŠ‚å’Œå®éªŒ")


if __name__ == "__main__":
    main()