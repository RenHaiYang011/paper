environment:  # 地图空间大小
  num_envs: 2
  seed: 3  # not 0, would mean same start position for every uav
  x_dim: 50 # [m]
  y_dim: 50 # [m]

sensor:
  type: "rgb_camera"
  field_of_view:
    angle_x: 60 # [°]
    angle_y: 60 # [°]
  pixel:
    number_x: 57   # 57 --> 10cm resolution
    number_y: 57
  encoding: "rgb8"
  model:
    type: "altitude_dependent"
    coeff_a: 0.05   # 0.2
    coeff_b: 0.2   # 0.5
  simulation:
    type: "random_field"
    cluster_radius: 5

mapping:
  prior: 0.5

# State Representation Configuration (新增状态表示配置)
state_representation:
  use_discovery_history: true        # 使用目标发现历史特征
  use_exploration_intensity: true    # 使用探索强度特征
  use_target_probability: true       # 使用目标存在概率图
  exploration_decay_factor: 0.9      # 探索强度时间衰减因子
  discovery_influence_radius: 2      # 发现目标的影响半径

experiment:
  title: "test_experiment"
  constraints:
    spacing: 5 # [m]
    min_altitude: 5
    max_altitude: 15
    budget: 14
    num_actions: 6
  uav:
    max_v: 5                 # [m/s]
    max_a: 2                 # [m/s]
    sampling_time: 2         # [s]
    communication_range: 25  # [m]
    fix_range: True
    failure_rate: 0
  missions:
    type: "COMA"              # COMA, random or lawnmower, or DeepQ
    mission_mode: "training"     # training or deployment
    n_episodes: 1500
    patience: 100
    n_agents: 4
    action_space: "larger"      # smaller, larger, reduced, 2d_reduced or one_altitude
    planning_uncertainty: "SE"         # SE, RMSE or MAE
    eps_max: 0.5
    eps_min: 0.02
    eps_anneal_phase: 10000  # 15000
    use_eps: True
    class_weighting: [0, 1]
    reward_normalization: false
  coverage_weight: 0.15   # 区域覆盖权重设置；信息应用+奖励多覆盖
  distance_weight: 0.0    # 距离惩罚权重：每步按距离扣分（保持0以允许远距离探索）
  footprint_weight: 0.5   # 足迹重叠惩罚权重（中等惩罚避免过度重叠）
  collision_weight: 2.0   # 碰撞惩罚权重（强力惩罚确保避碰）
  collision_distance: 5.0 # 碰撞判定阈值（米），等于一个移动步长提供安全距离
  altitude_diversity_weight: 0.5  # 高度多样性奖励权重: 鼓励agents在不同高度探索以获得更好的传感器性能
  
  # Target Discovery Rewards (新增目标发现奖励机制)
  target_discovery_reward: 50.0     # 首次发现目标的巨大正奖励
  mission_success_reward: 100.0     # 任务成功完成奖励
  mission_failure_penalty: -50.0    # 任务失败惩罚
  collaborative_discovery_weight: 25.0  # 协同发现奖励权重
  discovery_threshold: 0.8           # 目标发现的置信度阈值
  
  baselines:
    lawnmower:
      trials: 50
      altitude: 5
    random:
      n_episodes: 50
    information_gain:
      trials: 50
      communication: true

evaluation:
  repetitions: 1
  metrics:
    - "num_waypoints"
    - "paths"
    - "rmse"
    - "wrmse"
    - "mll"
    - "wmll"
    - "run_time"

networks:
  device: "cuda" # "cuda" or "cpu"
  type: "CNN"
  # 优化后的配置：减少训练步数，提高训练速度
  data_passes: 3      # 从5减到3
  batch_size: 64      # 从128减到64，减少内存使用和计算时间
  batch_number: 3     # 从5减到3
  # data_passes: 1
  # batch_size: 16
  # batch_number: 3
  copy_rate: 10     # full episodes
  gamma: 0.99
  lambda: 0.8
  actor:
    hidden_dim: 128
    learning_rate: 0.00001
    momentum: 0.9
    gradient_norm: 10
  critic:
    target_update_mode: "hard"
    tau: 0.01
    update_mode: "random_batches"
    synchronization: "no"
    fc1_dim: 64
    learning_rate: 0.0001
    momentum: 0.9
    gradient_norm: 10

classification:
  n_episodes: 300
  data_split: [0.4, 0.4, 0.2]
  number_epochs: 100
  batch_size: 50
