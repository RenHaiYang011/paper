 # 高级静态搜索配置 - 整合深度创新机制
# 基于论文创新点: 稀疏奖励、协同机制、搜索专用状态表示

environment:
  num_envs: 2
  seed: 3
  x_dim: 50
  y_dim: 50

sensor:
  type: "rgb_camera"
  field_of_view:
    angle_x: 60
    angle_y: 60
  pixel:
    number_x: 57
    number_y: 57
  encoding: "rgb8"
  model:
    type: "altitude_dependent"
    coeff_a: 0.05
    coeff_b: 0.2
  simulation:
    type: "random_field"
    cluster_radius: 5

mapping:
  prior: 0.5

# ==================== 搜索区域定义 ====================
search_regions:
  regions:
    - name: "high_priority_zone"
      type: "rectangle"
      priority: 1.0
      min_coverage: 0.95
      coordinates: [[10, 10, 25, 25]]
      search_density: "high"
      # 目标可能性分布 (用于先验信息实验)
      target_probability: 0.6  # 60%的目标可能在此区域
      
    - name: "medium_priority_zone"
      type: "rectangle"
      priority: 0.6
      min_coverage: 0.85
      coordinates: [[30, 10, 45, 25]]
      search_density: "medium"
      target_probability: 0.3
      
    - name: "low_priority_zone"
      type: "rectangle"
      priority: 0.3
      min_coverage: 0.70
      coordinates: [[10, 30, 45, 45]]
      search_density: "low"
      target_probability: 0.1
  
  strategy:
    mode: "priority_based"
    allow_overlap: false
    revisit_threshold: 0.1
    completion_threshold: 0.90
  
  density_requirements:
    high: 3
    medium: 2
    low: 1

experiment:
  title: "advanced_search_with_intrinsic_rewards"
  constraints:
    spacing: 5
    min_altitude: 5
    max_altitude: 25
    budget: 12
    num_actions: 6
  
  uav:
    max_v: 5
    max_a: 2
    sampling_time: 2
    communication_range: 25
    fix_range: True
    failure_rate: 0
  
  missions:
    type: "COMA"
    mission_mode: "training"
    n_episodes: 1000
    patience: 150
    n_agents: 4
    action_space: "larger"
    planning_uncertainty: "SE"
    eps_max: 0.5
    eps_min: 0.02
    eps_anneal_phase: 10000
    use_eps: True
    class_weighting: [0, 1]
    reward_normalization: false
  
  # ==================== 创新点1: 内在奖励机制 (解决稀疏奖励) ====================
  intrinsic_rewards:
    enable: true
    
    # 1.1 基于覆盖率的驱动 (Coverage-driven)
    coverage_exploration_weight: 0.8
    # 奖励访问低覆盖区域,鼓励探索未搜索区域
    coverage_decay_factor: 0.95  # 覆盖衰减因子,模拟"遗忘"
    
    # 1.2 前沿探测驱动 (Frontier-based)
    frontier_reward_weight: 1.0
    # 奖励靠近已探索/未探索边界的行为
    frontier_detection_threshold: 0.3  # 覆盖率阈值,用于识别前沿
    
    # 1.3 基于好奇心的驱动 (Curiosity-driven)
    curiosity_weight: 0.5
    # 奖励预测误差大的区域(未来可扩展为RND或ICM)
    prediction_horizon: 3  # 预测未来N步的观测
    
    # 1.4 搜索完备性奖励
    completeness_weight: 0.3
    # 基于搜索置信度图的奖励
    confidence_threshold: 0.8  # 搜索置信度阈值
  
  # ==================== 创新点2: 协同机制与信用分配 ====================
  coordination:
    enable: true
    
    # 2.1 抗重叠惩罚 (Anti-overlap Penalty)
    overlap_penalty_weight: 1.5
    overlap_threshold: 0.3  # 路径/观测重叠度阈值
    
    # 2.2 区域分工奖励 (Division-of-labor Reward)
    division_reward_weight: 0.8
    # 奖励智能体分散到不同区域搜索
    region_assignment_bonus: 1.0  # 当智能体在不同区域时的奖励
    
    # 2.3 协同发现奖励 (Collaborative Discovery)
    joint_discovery_weight: 2.0
    # 当多个智能体协同搜索同一高优先级区域时的奖励
    collaboration_distance: 15.0  # 协同距离阈值
    
    # 2.4 通信效率
    communication_cost: 0.01  # 通信成本(用于研究通信约束)
    
  # ==================== 传统奖励权重 (降低以突出内在奖励) ====================
  coverage_weight: 0.05
  distance_weight: 0.0
  footprint_weight: 0.1
  collision_weight: 2.0
  collision_distance: 5.0
  altitude_diversity_weight: 0.2
  
  # 区域搜索奖励 (保留)
  region_coverage_weight: 0.8
  region_priority_weight: 0.6
  search_density_weight: 0.5
  search_completion_weight: 1.5
  redundant_search_penalty: -0.8
  region_transition_penalty: -0.2
  
  baselines:
    lawnmower:
      trials: 50
      altitude: 5
    random:
      n_episodes: 50
    information_gain:
      trials: 50
      communication: true

# ==================== 创新点3: 搜索专用状态表示 ====================
state_representation:
  # 3.1 搜索置信度图 (Search Confidence Map)
  use_confidence_map: true
  confidence_update_rate: 0.1
  
  # 3.2 联合搜索前景图 (Joint Search Prospect Map)
  use_joint_prospect: true
  prospect_fusion_method: "max"  # max, mean, weighted
  
  # 3.3 前沿图 (Frontier Map)
  use_frontier_map: true
  frontier_kernel_size: 3
  
  # 3.4 预测误差图 (用于好奇心驱动)
  use_prediction_error_map: false  # 未来扩展

evaluation:
  repetitions: 1
  
  # ==================== 搜索专用评估指标 ====================
  metrics:
    # 基础指标
    - "num_waypoints"
    - "paths"
    - "rmse"
    - "wrmse"
    - "mll"
    - "wmll"
    - "run_time"
    
    # 搜索核心指标
    - "first_target_discovery_time"      # 首次发现目标时间
    - "average_discovery_time"           # 平均目标发现时间
    - "task_completion_time"             # 任务完成时间
    - "final_discovery_rate"             # 最终目标发现率
    
    # 搜索效率指标
    - "coverage_curve"                   # 覆盖率随时间变化
    - "path_redundancy"                  # 路径重复度
    - "overlap_degree"                   # 重叠度
    - "search_efficiency"                # 搜索效率 = 覆盖/时间
    
    # 协同效能指标
    - "coordination_efficiency"          # 协同效率
    - "load_balance"                     # 负载均衡度
    - "communication_overhead"           # 通信开销
    
    # 区域搜索指标
    - "region_coverage_rate"
    - "priority_satisfaction"
    - "redundant_search_ratio"

networks:
  device: "cuda"
  type: "CNN"
  data_passes: 2
  batch_size: 32
  batch_number: 2
  copy_rate: 10
  gamma: 0.99
  lambda: 0.8
  
  actor:
    hidden_dim: 128
    learning_rate: 0.00001
    momentum: 0.9
    gradient_norm: 10
  
  critic:
    target_update_mode: "hard"
    tau: 0.01
    update_mode: "random_batches"
    synchronization: "no"
    fc1_dim: 64
    learning_rate: 0.0001
    momentum: 0.9
    gradient_norm: 10

classification:
  n_episodes: 300
  data_split: [0.4, 0.4, 0.2]
  number_epochs: 100
  batch_size: 50

# ==================== 实验设计配置 ====================
experiments:
  # 消融实验配置
  ablation_studies:
    enable: true
    
    # 实验1: 内在奖励消融
    intrinsic_reward_ablation:
      - baseline: "no_intrinsic"           # 无内在奖励
      - coverage_only: "coverage_driven"   # 仅覆盖驱动
      - frontier_only: "frontier_driven"   # 仅前沿驱动
      - curiosity_only: "curiosity_driven" # 仅好奇心驱动
      - full: "all_intrinsic"              # 全部内在奖励
    
    # 实验2: 协同机制消融
    coordination_ablation:
      - no_penalty: "no_overlap_penalty"   # 无抗重叠惩罚
      - no_division: "no_division_reward"  # 无分工奖励
      - no_collab: "no_collaboration"      # 无协同发现
      - full: "full_coordination"          # 完整协同机制
    
    # 实验3: 通信条件分析
    communication_ablation:
      - full_comm: 25.0      # 完全通信 (25m范围)
      - limited_comm: 15.0   # 受限通信 (15m范围)
      - sparse_comm: 10.0    # 稀疏通信 (10m范围)
      - no_comm: 0.0         # 无通信
  
  # 基准测试场景
  benchmark_scenarios:
    enable: true
    
    # 场景1: 不同地图大小
    map_sizes:
      - small: [30, 30]
      - medium: [50, 50]
      - large: [70, 70]
    
    # 场景2: 不同障碍物密度
    obstacle_densities:
      - none: 0.0
      - low: 0.1
      - medium: 0.2
      - high: 0.3
    
    # 场景3: 不同目标分布
    target_distributions:
      - uniform: "uniform"
      - clustered: "clustered"
      - random: "random"
    
    # 场景4: 不同先验信息
    prior_conditions:
      - no_prior: "uniform_prior"
      - partial_prior: "region_based_prior"
      - full_prior: "target_probability_map"

# ==================== 配置说明 ====================
# 
# 本配置整合了三大创新点:
# 
# 1. 稀疏奖励解决方案:
#    - 覆盖探索驱动: 奖励访问未搜索区域
#    - 前沿探测驱动: 奖励探索已知/未知边界
#    - 好奇心驱动: 奖励预测误差大的区域
#    - 搜索完备性: 基于置信度图的奖励
# 
# 2. 协同机制改进:
#    - 抗重叠惩罚: 避免路径/观测重复
#    - 区域分工: 鼓励智能体分散搜索
#    - 协同发现: 奖励多机协同搜索高优先级区域
#    - 通信成本: 研究通信约束下的性能
# 
# 3. 搜索专用状态:
#    - 搜索置信度图: 每个网格的搜索完备程度
#    - 联合搜索前景图: 融合多机观测的搜索图
#    - 前沿图: 已探索/未探索边界
# 
# 评估体系:
#    - 核心指标: 发现时间、完成率
#    - 效率指标: 覆盖曲线、路径重复度
#    - 协同指标: 协同效率、负载均衡
# 
# 消融实验:
#    - 内在奖励各组件贡献
#    - 协同机制各组件贡献
#    - 不同通信条件下的性能
