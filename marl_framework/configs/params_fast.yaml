# 快速训练配置 - 优化GPU利用率 + 完整新设计功能
# 使用方法: export CONFIG_FILE_PATH=configs/params_fast.yaml && python main.py
# 
# 🚀 新增功能:
# - ✅ 区域搜索管理 (SearchRegionManager)
# - ✅ 前沿探测奖励 (FrontierManager) 
# - ✅ 目标发现奖励 (巨大奖励+50.0)
# - ✅ 协同机制 (CoordinationManager)
# - ✅ 9层状态表示 (概率图+发现历史+探索强度)
# - ✅ 多层奖励架构 (解决稀疏奖励问题)
#
# ⚡ 快速训练参数:
# - Budget: 8步 (vs 标准14步)
# - Episodes: 500 (vs 标准1500)
# - 预计时间: 8-12小时
# - 模型性能: ~80-85%

# ==================== 路径配置 ====================
paths:
  # 日志存储路径 (可以是绝对路径或相对于marl_framework的路径)
  log_dir: "log"                    # 默认: marl_framework/log
  # 结果存储路径 (可以是绝对路径或相对于marl_framework的路径)  
  results_dir: "res"                # 默认: marl_framework/res
  
  # 示例其他配置:
  # log_dir: "E:/my_logs"           # 绝对路径
  # results_dir: "../paper_results" # 相对于marl_framework的上级目录
  # log_dir: "/tmp/marl_logs"       # Linux绝对路径

environment:
  num_envs: 2
  seed: 3
  x_dim: 50
  y_dim: 50

sensor:
  type: "rgb_camera"
  field_of_view:
    angle_x: 60
    angle_y: 60
  pixel:
    number_x: 57
    number_y: 57
  encoding: "rgb8"
  model:
    type: "altitude_dependent"
    coeff_a: 0.05
    coeff_b: 0.2
  simulation:
    type: "random_field"
    cluster_radius: 5

mapping:
  prior: 0.5

# ==================== 搜索区域定义 ====================
search_regions:
  regions:
    - name: "high_priority_zone"
      type: "rectangle"
      priority: 1.0
      min_coverage: 0.90
      coordinates: [[10, 10, 25, 25]]
      search_density: "high"
      target_probability: 0.6
      
    - name: "medium_priority_zone"
      type: "rectangle"
      priority: 0.6
      min_coverage: 0.80
      coordinates: [[30, 10, 45, 25]]
      search_density: "medium"
      target_probability: 0.3
      
    - name: "low_priority_zone"
      type: "rectangle"
      priority: 0.3
      min_coverage: 0.65
      coordinates: [[10, 30, 45, 45]]
      search_density: "low"
      target_probability: 0.1
  
  strategy:
    mode: "priority_based"
    allow_overlap: false
    revisit_threshold: 0.1
    completion_threshold: 0.85
  
  density_requirements:
    high: 3
    medium: 2
    low: 1

experiment:
  title: "fast_training"
  constraints:
    spacing: 5       # 必须为整数，以优化计算
    min_altitude: 5
    max_altitude: 25
    budget: 8                # ← 从14减到8，大幅减少CPU计算
    num_actions: 6
  uav:
    max_v: 5
    max_a: 2
    sampling_time: 2
    communication_range: 25
    fix_range: True
    failure_rate: 0
  missions:
    type: "COMA"
    mission_mode: "training"
    n_episodes: 500          # ← 从1500减到500，快速测试
    patience: 100
    n_agents: 4
    action_space: "larger"
    planning_uncertainty: "SE"
    eps_max: 0.5
    eps_min: 0.02
    eps_anneal_phase: 10000
    use_eps: True
    class_weighting: [0, 1] 
    reward_normalization: false
  coverage_weight: 0.15
  distance_weight: 0.0
  footprint_weight: 0.5
  collision_weight: 2.0
  collision_distance: 5.0
  altitude_diversity_weight: 0.5  # 高度多样性奖励权重: 鼓励agents在不同高度探索
  
  # ==================== 新设计1: 内在奖励机制 (解决稀疏奖励) ====================
  intrinsic_rewards:
    enable: true
    
    # 1.1 前沿探测驱动 (核心创新)
    frontier_reward_weight: 1.0
    frontier_detection_threshold: 0.3
    
    # 1.2 覆盖率驱动
    coverage_exploration_weight: 0.6
    coverage_decay_factor: 0.95
    
    # 1.3 搜索完备性奖励
    completeness_weight: 0.3
    confidence_threshold: 0.8
  
  # ==================== 新设计2: 协同机制 ====================
  coordination:
    enable: true
    
    # 抗重叠惩罚
    overlap_penalty_weight: 1.2
    overlap_threshold: 0.3
    
    # 区域分工奖励
    division_reward_weight: 0.6
    region_assignment_bonus: 0.8
    
    # 协同发现奖励
    joint_discovery_weight: 1.5
    collaboration_distance: 15.0
    
    # 通信效率
    communication_cost: 0.01
  
  # ==================== 新设计3: 目标发现奖励机制 ==================== 
  target_discovery_reward: 50.0     # 首次发现目标的巨大正奖励
  mission_success_reward: 100.0     # 任务成功完成奖励
  mission_failure_penalty: -50.0    # 任务失败惩罚
  collaborative_discovery_weight: 25.0  # 协同发现奖励权重
  discovery_threshold: 0.8           # 目标发现的置信度阈值
  
  # ==================== 新设计4: 区域搜索奖励 ==================== 
  region_coverage_weight: 0.6       # 区域覆盖奖励权重
  region_priority_weight: 0.5       # 区域优先级奖励权重
  search_density_weight: 0.4        # 搜索密度奖励权重
  search_completion_weight: 1.0     # 搜索完成奖励权重
  redundant_search_penalty: -0.6    # 冗余搜索惩罚
  region_transition_penalty: -0.2   # 区域转换惩罚
  
  baselines:
    lawnmower:
      trials: 50
      altitude: 5
    random:
      n_episodes: 50
    information_gain:
      trials: 50
      communication: true

# ==================== 新设计5: 搜索专用状态表示 ====================
state_representation:
  # 发现历史图
  use_discovery_history: true
  discovery_influence_radius: 2
  
  # 探索强度图  
  use_exploration_intensity: true
  exploration_decay_factor: 0.9
  
  # 目标概率图
  use_target_probability: true
  
  # 搜索置信度图
  use_confidence_map: true
  confidence_update_rate: 0.1
  
  # 前沿图
  use_frontier_map: true
  frontier_kernel_size: 3
  
  # 联合搜索前景图
  use_joint_prospect: true
  prospect_fusion_method: "max"

evaluation:
  repetitions: 1
  metrics:
    # 基础指标
    - "num_waypoints"
    - "paths"
    - "rmse"
    - "wrmse"
    - "mll" 
    - "wmll"
    - "run_time"
    # 搜索专用指标
    - "target_discovery_rate"
    - "search_efficiency"
    - "collaboration_index"
    - "region_completion_rate"

networks:
  device: "cuda"
  type: "CNN"
  data_passes: 2           # ← 从3减到2
  batch_size: 32           # ← 从64减到32
  batch_number: 2          # ← 从3减到2
  copy_rate: 10
  gamma: 0.99
  lambda: 0.8
  actor:
    hidden_dim: 128
    learning_rate: 0.00001
    momentum: 0.9
    gradient_norm: 10
  critic:
    target_update_mode: "hard"
    tau: 0.01
    update_mode: "random_batches"
    synchronization: "no"
    fc1_dim: 64
    learning_rate: 0.0001
    momentum: 0.9
    gradient_norm: 10

classification:
  n_episodes: 300
  data_split: [0.4, 0.4, 0.2]
  number_epochs: 100
  batch_size: 50

# ==================== 可视化配置 ====================
visualization:
  # 障碍物配置 (可选 - 如果不配置会自动生成随机障碍物)
  # 用于在轨迹图中显示空中障碍物 (树木、山峰等)
  obstacles:
    - x: 15      # X坐标 (米)
      y: 25      # Y坐标 (米)
      z: 0       # 底部高度 (米, 通常为0)
      height: 12 # 障碍物高度 (米)
    
    - x: 35
      y: 15
      z: 0
      height: 15
    
    - x: 25
      y: 40
      z: 0
      height: 10
  
  # 未来可扩展:
  # - 动态障碍物
  # - 风场可视化
  # - 通信范围显示

# ==================== 配置说明 ====================
# 
# 🎯 新设计规划方法完整集成:
# 
# 1. 稀疏奖励解决方案:
#    - 目标发现奖励: +50.0 (巨大即时奖励)
#    - 前沿探测奖励: 持续探索激励
#    - 协同发现奖励: 团队合作奖励
#    - 任务完成奖励: +100.0 终极目标
# 
# 2. 协同机制改进:
#    - 抗重叠惩罚: 避免路径/观测重复
#    - 区域分工: 鼓励智能体分散搜索
#    - 协同发现: 奖励多机协同搜索高优先级区域
#    - 通信成本: 研究通信约束下的性能
# 
# 3. 搜索专用状态表示 (9层输入):
#    - 预算图 + 位置图 + 智能体ID图
#    - 加权熵图 + 局部熵图 + 概率图 ⭐
#    - 足迹图 + 发现历史图 ⭐ + 探索强度图 ⭐
#
# 4. 区域搜索管理:
#    - 3个优先级区域 (高/中/低)
#    - 区域分工和协调
#    - 搜索密度控制
#    - 完成度跟踪
#
# 5. 前沿探测系统:
#    - 自动识别探索边界
#    - 距离奖励引导
#    - 前沿图状态表示
#
# 快速训练优化:
# - budget: 8 (减少每个episode的步数，减轻CPU压力)
# - n_episodes: 500 (快速验证训练效果)  
# - batch_size: 32 (减少内存和计算压力)
# - data_passes: 2 (加快训练循环)
#
# 预期效果:
# - 总训练步数: ~2,083步
# - 预计时间: 8-12小时 (包含新功能)
# - GPU利用率: 20-40%
# - 模型性能: 80-85% (相比完整配置)
# - 新功能验证: 完整覆盖所有创新点
